{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T12:40:40.206110Z",
     "start_time": "2018-11-21T12:40:39.699140Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from preprocessing import *\n",
    "from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T12:40:40.664440Z",
     "start_time": "2018-11-21T12:40:40.587192Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open('X.p','rb'))\n",
    "y = pickle.load(open('y.p','rb'))\n",
    "nas = pickle.load(open('nas.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train, Val, and Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T12:40:41.830292Z",
     "start_time": "2018-11-21T12:40:41.735437Z"
    }
   },
   "outputs": [],
   "source": [
    "# We use the first half of the dataset for training, the next 25% for validation and the final 25% for testing\n",
    "\n",
    "n_trn = len(X) // 2\n",
    "n_valid = n_trn + (len(X) // 4)\n",
    "X_train, X_valid, X_test = split_vals_test(X, n_trn, n_valid)\n",
    "y_train, y_valid, y_test = split_vals_test(y, n_trn, n_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T12:40:56.709435Z",
     "start_time": "2018-11-21T12:40:46.647991Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.4 s, sys: 170 ms, total: 39.5 s\n",
      "Wall time: 7.93 s\n",
      "[0.07975016230858385, 0.20360450275845207, 0.972181703926252, 0.8275231910300023, -6.274355518398083]\n"
     ]
    }
   ],
   "source": [
    "m = RFR(n_jobs=-1, oob_score=True)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**:  r^2 of 0.83.  Negative oob score indicates that we are not using enough estimators (decision trees) in our forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:22.331Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.8 s, sys: 228 ms, total: 39 s\n",
      "Wall time: 7.71 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefan/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:724: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 1.34 s, total: 1min 20s\n",
      "Wall time: 13.2 s\n",
      "CPU times: user 3min 15s, sys: 577 ms, total: 3min 16s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "# n_estimators\n",
    "\n",
    "m1 = RFR(n_estimators=10, n_jobs=-1, oob_score=True)\n",
    "m2 = RFR(n_estimators=20, n_jobs=-1, oob_score=True)\n",
    "m3 = RFR(n_estimators=40, n_jobs=-1, oob_score=True)\n",
    "m4 = RFR(n_estimators=80, n_jobs=-1, oob_score=True)\n",
    "%time m1.fit(X_train, y_train)\n",
    "%time m2.fit(X_train, y_train)\n",
    "%time m3.fit(X_train, y_train)\n",
    "%time m4.fit(X_train, y_train)\n",
    "print_score(m1, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m2, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m3, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m4, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** The returns on adding estimators are reduced significantly (judging from oob) when going from 40 to 80.  Thus, we'll stick with 40 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:23.018Z"
    }
   },
   "outputs": [],
   "source": [
    "# min_samples_leaf\n",
    "\n",
    "m1 = RFR(n_estimators=40, min_samples_leaf=3, n_jobs=-1, oob_score=True)\n",
    "m2 = RFR(n_estimators=40, min_samples_leaf=5, n_jobs=-1, oob_score=True)\n",
    "m3 = RFR(n_estimators=40, min_samples_leaf=10, n_jobs=-1, oob_score=True)\n",
    "m4 = RFR(n_estimators=40, min_samples_leaf=25, n_jobs=-1, oob_score=True)\n",
    "%time m1.fit(X_train, y_train)\n",
    "%time m2.fit(X_train, y_train)\n",
    "%time m3.fit(X_train, y_train)\n",
    "%time m4.fit(X_train, y_train)\n",
    "print_score(m1, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m2, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m3, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m4, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** Going from 1 to 3 (1 was the default value from the previous cell) we see the oob and validation r^2 improving slightly, so we'll go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:23.313Z"
    }
   },
   "outputs": [],
   "source": [
    "# max_feaures\n",
    "\n",
    "m1 = RFR(n_estimators=40, min_samples_leaf=3, max_features=1, n_jobs=-1, oob_score=True)\n",
    "m2 = RFR(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m3 = RFR(n_estimators=40, min_samples_leaf=3, max_features='sqrt', n_jobs=-1, oob_score=True)\n",
    "m4 = RFR(n_estimators=40, min_samples_leaf=3, max_features='log2', n_jobs=-1, oob_score=True)\n",
    "%time m1.fit(X_train, y_train)\n",
    "%time m2.fit(X_train, y_train)\n",
    "%time m3.fit(X_train, y_train)\n",
    "%time m4.fit(X_train, y_train)\n",
    "print_score(m1, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m2, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m3, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m4, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** max_features = 0.5 is the clear winner here.  I'll now re-train the forest with more estimators to see if that yields any additional gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:25.311Z"
    }
   },
   "outputs": [],
   "source": [
    "# max_feaures\n",
    "\n",
    "m1 = RFR(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m2 = RFR(n_estimators=60, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m3 = RFR(n_estimators=80, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "m4 = RFR(n_estimators=100, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n",
    "%time m1.fit(X_train, y_train)\n",
    "%time m2.fit(X_train, y_train)\n",
    "%time m3.fit(X_train, y_train)\n",
    "%time m4.fit(X_train, y_train)\n",
    "print_score(m1, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m2, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m3, X_train, y_train, X_valid, y_valid)\n",
    "print_score(m4, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** we see some minor improvement going from 40 to 80, but above that the changes are really negligible.  We'll stick with 80 as our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:26.144Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = m3\n",
    "pickle.dump(best_model, open('nb5_best.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:26.662Z"
    }
   },
   "outputs": [],
   "source": [
    "best = pickle.load(open('nb5_best.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-11-21T12:41:27.402Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump([X_train, y_train], open('train.p','wb'))\n",
    "pickle.dump([X_valid, y_valid], open('valid.p','wb'))\n",
    "pickle.dump([X_test, y_test], open('test.p','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
